{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17eeOwO6zrNE9N_hMTwtYgjY5qz-NC1y0","timestamp":1669384569983},{"file_id":"1oBSJardm70rn2mzlYEfqHEmi-nFuD9lW","timestamp":1668505491039}],"mount_file_id":"17eeOwO6zrNE9N_hMTwtYgjY5qz-NC1y0","authorship_tag":"ABX9TyPyTaWj8FHZMaCrFhZBNP+I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k_hrj1-zqnu4"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import re"]},{"cell_type":"code","source":["# Read data from my drive\n","\n","original_data = '/content/drive/MyDrive/CS3244 Dataset/train-balanced-sarcasm.csv'\n","\n","cleaned_dataset = [\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/no_pre_processing.csv',\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/lowercased.csv', \n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_no_stopwords_refined.csv',\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_no_stopwords.csv',\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_lemmatized.csv',\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_default_lemmatized.csv'\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/no_puncutation.csv', \n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/no_punctuation_numeric_empty.csv',\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/everything.csv',\n","    '/content/drive/MyDrive/CS3244 Dataset/pre-processed/everything_except_punctuation.csv',\n","]\n","\n","datastore = pd.read_csv(cleaned_dataset[0])\n","datastore = datastore.dropna()    \n","total_count = len(datastore)\n","print(versions.split(\"/\")[-1])\n","print(\"Length of dataset: \", total_count)\n","\n","# \n","to_tokenize = '!\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n.,:;!?'\n","datastore['comment'] = datastore['comment'].apply(lambda x: re.sub(r'(['+to_tokenize+'])', r' \\1 ', x))\n","\n","label = datastore['label'].tolist()\n","comment = datastore['comment'].tolist()\n","meta_data = np.array([np.array([float(num) for num in stats.split(\" \")]) for stats in datastore['meta_features']])\n","index = [i for i in range(len(comment))]\n","\n","# Prepare the data by splitting into test and train\n","from sklearn.model_selection import train_test_split\n","X_train_ref, X_test_ref, y_train, y_test = train_test_split(index, label, train_size = 0.8, random_state = 42, shuffle = True)\n","datastore.shape\n","\n","X_train = [comment[index] for index in X_train_ref]\n","X_test = [comment[index] for index in X_test_ref]\n","X_train_meta = [meta_data[index] for index in X_train_ref]\n","X_test_meta = [meta_data[index] for index in X_test_ref]\n","\n","#tokenize text\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Embedding, Concatenate, Input, concatenate, Conv1D, GlobalMaxPooling1D\n","from keras import Input, Model\n","import pickle\n","\n","vocab_size = 10000\n","embedding_dim = 16\n","max_length = 120\n","\n","# Prepare tokenizer and save it to drive\n","tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\", filters='',lower=False)\n","tokenizer.fit_on_texts(X_train)\n","\n","# with open('/content/drive/MyDrive/CS3244 Dataset/tokenizer.pickle', 'wb') as handle:\n","#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","training_sequences = tokenizer.texts_to_sequences(X_train)\n","training_padded = pad_sequences(training_sequences,maxlen=max_length, truncating='post')\n","testing_sequences = tokenizer.texts_to_sequences(X_test)\n","testing_padded = pad_sequences(testing_sequences, maxlen=max_length, truncating='post')\n","\n","training_padded = np.array(training_padded)\n","training_labels = np.array(y_train)\n","testing_padded = np.array(testing_padded)\n","testing_labels = np.array(y_test)\n","training_meta = np.array(X_train_meta)\n","testing_meta = np.array(X_test_meta)\n","\n","# CNN Model definition\n","nlp_input = Input(shape=(max_length,)) \n","meta_input = Input(shape=(5,))\n","embed = Embedding(vocab_size,\n","                  embedding_dim,\n","                  input_length=max_length)(nlp_input)\n","nlp_out = Conv1D(128, 5, activation='relu')(embed)\n","max_pool = GlobalMaxPooling1D()(nlp_out)\n","concat = concatenate([max_pool, meta_input], axis=1)\n","middle = Dense(3, activation='relu')(concat)\n","output = Dense(1, activation='sigmoid')(middle)\n","model = Model(inputs=[nlp_input , meta_input], outputs=[output])\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()\n","history = model.fit([training_padded, training_meta], training_labels, epochs=10, batch_size=32, validation_data=([testing_padded, testing_meta], testing_labels), verbose=1,  callbacks=[\n","        tf.keras.callbacks.EarlyStopping(\n","            monitor='val_accuracy',\n","            patience=3,\n","            restore_best_weights=True\n","        )\n","    ])\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","\n","predictions = model.predict([testing_padded, testing_meta])\n","prediction_result = (predictions) > 0.5).astype('int32')\n","\n","# accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(testing_labels, prediction_result)\n","print('Accuracy: %f' % accuracy)\n","# precision tp / (tp + fp)\n","precision = precision_score(testing_labels, prediction_result)\n","print('Precision: %f' % precision)\n","# recall: tp / (tp + fn)\n","recall = recall_score(testing_labels, prediction_result)\n","print('Recall: %f' % recall)\n","# f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(testing_labels, prediction_result)\n","print('F1 score: %f' % f1)\n","# auc\n","auc = roc_auc_score(testing_labels, predictions)\n","print('AUC : %f\\n\\n' % auc)\n","\n","# Run only if you want to replace the saved model with current fit\n","# model.save('/content/drive/MyDrive/CS3244 Dataset')\n","\n","#Evaluating Accuracy and Loss of the model\n","%matplotlib inline\n","acc=history.history['accuracy']\n","val_acc=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) #No. of epochs\n","\n","#Plot training and validation accuracy per epoch\n","import matplotlib.pyplot as plt\n","plt.plot(epochs,acc,'r',label='Training Accuracy')\n","plt.plot(epochs,val_acc,'g',label='Testing Accuracy')\n","plt.legend()\n","plt.figure()\n","\n","#Plot training and validation loss per epoch\n","plt.plot(epochs,loss,'r',label='Training Loss')\n","plt.plot(epochs,val_loss,'g',label='Testing Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"nU8EOAGJuA6O"},"execution_count":null,"outputs":[]}]}