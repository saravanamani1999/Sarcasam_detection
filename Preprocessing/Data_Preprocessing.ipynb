{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FF0c7frgPe3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from my drive\n",
        "datastore = pd.read_csv('/content/drive/MyDrive/CS3244 Dataset/train-balanced-sarcasm.csv')\n",
        "total_count = len(datastore)"
      ],
      "metadata": {
        "id": "Hr_6d4NvgX6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string, time\n",
        "from nltk.corpus import stopwords as stopwordprovider\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\"\"\"\n",
        "Sequence of pre-processing\n",
        "\n",
        "1. Make a copy of the dataframe. All modification will only be done on the duplicate\n",
        "2. Iterate through each element in df and check if comment or label is NULL. Drop the row if comment or label is missing\n",
        "3. Remove punctuation\n",
        "4. Lowercase all words\n",
        "5. Tokenize sentence into small units ( usually single words )\n",
        "6. Remove stopwords         # decided against it as it removes sentimental information\n",
        "7. Stemming the words\n",
        "8. Lemmatizing words\n",
        "9. Iterate through each element and drop if only contains numerics\n",
        "10. Return a new DataFrame containing only the cleaned comments and labels\n",
        "\"\"\"\n",
        "\n",
        "# Helper methods\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "def tokenization(text):\n",
        "    tokens = re.split(\"\\W+\",text)\n",
        "    return tokens   \n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # stopwords = stopwords[0:10]       # You can shrink down the list of stopwords that you want removed by slicing stopword \n",
        "    output= [i for i in text if i not in stopwords or i in unsafe_stopwords]\n",
        "    return output\n",
        "\n",
        "def stemming(text):\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text\n",
        "\n",
        "def lemmatizer(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    endings = [\"er\", \"est\"]\n",
        "    lemm_text = [word if any(word.endswith(ending) for ending in endings) else wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text\n",
        "\n",
        "def default_lemmatizer(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text\n",
        "\n",
        "def printDropNaNSummary(comment_initial, comment_final, label_initial, label_final):\n",
        "  no_of_dropped_comments = comment_initial - comment_final\n",
        "  print(\"Found {} missing data entry with NaN values in either col_name or 'label'\".format(no_of_dropped_comments))\n",
        "  print(\"\\nAfter dropping missing and duplicate data:\\n{} col --> {} col \\n{} labels --> {} labels\\n\".format(comment_initial, \n",
        "                                                                                                        comment_final, \n",
        "                                                                                                        label_initial, \n",
        "                                                                                                        label_final))\n",
        "\n",
        "def alphabet_rep(text):\n",
        "  count = 0\n",
        "  letter = text[0]\n",
        "  for i in text:\n",
        "    if count > 2 :\n",
        "      return 1\n",
        "    if i == letter:\n",
        "      count += 1\n",
        "    else:\n",
        "      letter = i\n",
        "  return 0\n",
        "\n",
        "def exclamation_freq(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i == \"!\":\n",
        "      count += 1\n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def question_freq(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i == \"?\":\n",
        "      count += 1\n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def dots_freq(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i == \".\":\n",
        "      count += 1\n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def caps_freq(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i.isupper():\n",
        "      count += 1\n",
        "  return count/len(text)\n",
        "\n",
        "def quotes_freq(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i in  (\"\\'\" ,\"\\\"\"):\n",
        "      count += 1\n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def consecutive_caps(text):\n",
        "  n = len(text)\n",
        "  count = 0\n",
        "  for i in range(n):\n",
        "    if (i < n - 1 and text[i].isupper() and text[i+1].isupper()):\n",
        "      if i == 0 and text[i].isupper():\n",
        "        count += 1\n",
        "      if i > 0 and not text[i - 1].isupper():\n",
        "        count += 1\n",
        "      count += 1\n",
        "  \n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def consecutive_dots(text):\n",
        "  n = len(text)\n",
        "  count = 0\n",
        "  for i in range(n):\n",
        "    if (i < n - 1 and text[i] == \".\" and text[i+1] == \".\"):\n",
        "      if i == 0 and text[i] == \".\":\n",
        "        count += 1\n",
        "      if i > 0 and not text[i - 1] == \".\":\n",
        "        count += 1\n",
        "      count += 1\n",
        "  \n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def consecutive_exclamations(text):\n",
        "  n = len(text)\n",
        "  count = 0\n",
        "  for i in range(n):\n",
        "    if (i < n - 1 and text[i] == \"!\" and text[i+1] == \"!\"):\n",
        "      if i == 0 and text[i] == \"!\":\n",
        "        count += 1\n",
        "      if i > 0 and not text[i - 1] == \"!\":\n",
        "        count += 1\n",
        "      count += 1\n",
        "  \n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def consecutive_question(text):\n",
        "  n = len(text)\n",
        "  count = 0\n",
        "  for i in range(n):\n",
        "    if (i < n - 1 and text[i] == \"?\" and text[i+1] == \"?\"):\n",
        "      if i == 0 and text[i] == \"?\":\n",
        "        count += 1\n",
        "      if i > 0 and not text[i - 1] == \"?\":\n",
        "        count += 1\n",
        "      count += 1\n",
        "\n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "def consecutive_punctuations(text):\n",
        "  n = len(text)\n",
        "  count = 0\n",
        "  for i in range(n):\n",
        "    if (i < n - 1 and text[i] in string.punctuation and text[i+1] in string.punctuation):\n",
        "      if i == 0 and text[i] in string.punctuation:\n",
        "        count += 1\n",
        "      if i > 0 and not text[i - 1] in string.punctuation:\n",
        "        count += 1\n",
        "      count += 1\n",
        "\n",
        "  freq = count/len(text)\n",
        "  return freq\n",
        "\n",
        "# Apply the cleaning\n",
        "def text_preprocessing(df, col_name=\"comment\", size=\"all\", mode = [1,1,1,1,1,1,1]):\n",
        "  if col_name == \"\":\n",
        "    print(\"NO COL NAME SPECIFIED\")\n",
        "    return\n",
        "  if size == \"all\": size = df.size\n",
        "  df_split = df.head(size)\n",
        "  df_copy = df_split.copy()\n",
        "\n",
        "  initial_comment_length = df_copy[col_name].size\n",
        "  initial_label_length = df_copy[\"label\"].size\n",
        "\n",
        "  df_copy = df_copy.dropna()                                                # Drop rows with missing data in any column\n",
        "\n",
        "  comment_length_after_drop = df_copy[col_name].size\n",
        "  label_length_after_drop = df_copy['label'].size\n",
        "  if comment_length_after_drop != label_length_after_drop: return None     # Both series should have the same length\n",
        "\n",
        "  # Apply meta features\n",
        "  start = time.perf_counter()\n",
        "  df_copy['meta_features'] = df[\"comment\"].apply(lambda x: \"0 0 0 0 0 0 0 0 0\" if x != x else \n",
        "                                                 str(alphabet_rep(x)) + \" \" + \n",
        "                                                 str(exclamation_freq(x)) + \" \" +\n",
        "                                                 str(dots_freq(x)) + \" \" +\n",
        "                                                 str(question_freq(x)) + \" \" +\n",
        "                                                 str(quotes_freq(x)) + \" \" +\n",
        "                                                 str(consecutive_caps(x)) + \" \" + \n",
        "                                                 str(consecutive_exclamations(x)) + \" \" +\n",
        "                                                 str(consecutive_question(x)) + \" \" +\n",
        "                                                 str(consecutive_punctuations(x)))\n",
        "\n",
        "  end = time.perf_counter()\n",
        "  print(\"Completed extracting meta data ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Remove punctuation\n",
        "  if mode[0] == 1:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x:remove_punctuation(x))\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed removing punctuation ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Lowercase all words\n",
        "  if mode[1] == 1:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x: x.lower())\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed lowercasing ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Tokenize sentence into small units\n",
        "  if mode[2] == 1:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x: tokenization(x))\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed tokenizing ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Remove stopwords\n",
        "  if mode[3] == 1:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x:remove_stopwords(x))\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed removing stopwords ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Stemming the words\n",
        "  if mode[4] == 1:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x: stemming(x))\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed stemming ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Lemmatizing words\n",
        "  if mode[5] == 1:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x:lemmatizer(x))\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed lemmatizing ...... time taken: {}\".format(end-start))\n",
        "  elif mode[5] == 2:\n",
        "    start = time.perf_counter()\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x:default_lemmatizer(x))\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed lemmatizing ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Drop numeric and empty comments\n",
        "  if mode[6] == 1:\n",
        "    start = time.perf_counter()\n",
        "    mask = df_copy[col_name].apply(lambda x: all(y.isnumeric() for y in x))\n",
        "    og_length = len(df_copy)\n",
        "    indices = np.where(mask)\n",
        "    if len(indices[0]) > 0:\n",
        "      df_copy = df_copy.drop(df_copy.index[list(indices[0])])\n",
        "    end = time.perf_counter()\n",
        "    print(\"Completed dropping empty/numerical comments ...... time taken: {}\".format(end-start))\n",
        "\n",
        "  # Join if tokenized\n",
        "  if mode[2] == 1:\n",
        "    df_copy[col_name] = df_copy[col_name].apply(lambda x: ' '.join(x))\n",
        "\n",
        "  df_copy = df_copy.drop(['author',\t'subreddit',\t'score',\t'ups',\t'downs',\t'date',\t'created_utc',\t'parent_comment'], axis=1)\n",
        "  print('\\n***Completed***')\n",
        "  print('─' * 100)\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "yowkObfyggj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Order of array\n",
        "# -----------------------------------------------------------------\n",
        "# - Remove punctuation\n",
        "# - Lowercase all words\n",
        "# - Tokenize sentence into small units\n",
        "# - Remove stopwords\n",
        "# - Stemming the words\n",
        "# - Lemmatizing words\n",
        "# - Drop numeric and empty comments\n",
        "\n",
        "size = \"all\"\n",
        "\n",
        "# No pre-processing\n",
        "no_preprocessing = text_preprocessing(datastore, \"comment\", size=size, mode=[0,0,0,0,0,0,0])\n",
        "no_preprocessing.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/no_pre_processing.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Remove punctuation\n",
        "no_puncutation = text_preprocessing(datastore, \"comment\", size=size, mode=[1,0,0,0,0,0,0])\n",
        "no_puncutation.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/no_puncutation.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# # Lowercase and remove safe stopwords\n",
        "stopwords = stopwordprovider.words('english')\n",
        "stopwords = stopwords      # You can shrink down the list of stopwords that you want removed by slicing stopword \n",
        "unsafe_stopwords = ['not', 'you', 'so', 'just', 'because', 'but', 'if', 'as', 'no', 'only', 'more', 'how', 'than', 'too', 'i', 'me', 'such', 'most', 'very',  ]\n",
        "lowercase_no_stopwords_refined = text_preprocessing(datastore, \"comment\", size=size, mode=[0,1,1,1,0,0,0])\n",
        "lowercase_no_stopwords_refined.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_no_stopwords_refined.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Lowercase and remove first 10 stopwords\n",
        "stopwords = stopwordprovider.words('english')\n",
        "stopwords = stopwords[0:10]       # You can shrink down the list of stopwords that you want removed by slicing stopword \n",
        "lowercase_no_stopwords = text_preprocessing(datastore, \"comment\", size=size, mode=[0,1,1,1,0,0,0])\n",
        "lowercase_no_stopwords.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_no_stopwords.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Lowercase and lemmatized\n",
        "lowercase_lemmatized = text_preprocessing(datastore, \"comment\", size=size, mode=[0,1,1,0,0,1,0])\n",
        "lowercase_lemmatized.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_lemmatized.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Lowercase\n",
        "lowercased = text_preprocessing(datastore, \"comment\", size=size, mode=[0,1,1,0,0,0,0])\n",
        "lowercased.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/lowercased.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Remove punctuation and drop numeric/empty comments\n",
        "no_punctuation_numeric_empty = text_preprocessing(datastore, \"comment\", size=size, mode=[1,0,1,0,0,0,1])\n",
        "no_punctuation_numeric_empty.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/no_punctuation_numeric_empty.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Everything\n",
        "unsafe_stopwords = []\n",
        "everything = text_preprocessing(datastore, \"comment\", size=size, mode=[1,1,1,1,0,1,1])\n",
        "everything.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/everything.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Everything except punctuation\n",
        "everything_except_punctuation = text_preprocessing(datastore, \"comment\", size=size, mode=[0,1,1,1,0,1,1])\n",
        "everything_except_punctuation.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/everything_except_punctuation.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Everything-stemming\n",
        "unsafe_stopwords = []\n",
        "everything_stemming = text_preprocessing(datastore, \"comment\", size=size, mode=[1,1,1,1,1,0,1])\n",
        "everything_stemming.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/everything_stemming.csv\", encoding='utf-8', index=False)\n",
        "\n",
        "# Lowercase and default lemmatized\n",
        "lowercase_default_lemmatized = text_preprocessing(datastore, \"comment\", size=size, mode=[0,1,1,0,0,2,0])\n",
        "lowercase_default_lemmatized.to_csv(\"drive/MyDrive/CS3244 Dataset/pre-processed/lowercase_default_lemmatized.csv\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzlSN99ngpha",
        "outputId": "bbf14561-6f42-4661-ae9b-70158adbe5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed extracting meta data ...... time taken: 54.73973207899999\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 51.93829261399998\n",
            "Completed removing punctuation ...... time taken: 6.794852780000014\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 51.05047398000002\n",
            "Completed lowercasing ...... time taken: 0.3273212259999809\n",
            "Completed tokenizing ...... time taken: 6.38494426799997\n",
            "Completed removing stopwords ...... time taken: 29.041811886999994\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.696658592999995\n",
            "Completed lowercasing ...... time taken: 0.27651418199997124\n",
            "Completed tokenizing ...... time taken: 6.158338190000052\n",
            "Completed removing stopwords ...... time taken: 5.437610984000003\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.65385803800001\n",
            "Completed lowercasing ...... time taken: 0.2784923280000271\n",
            "Completed tokenizing ...... time taken: 6.066423455999995\n",
            "Completed lemmatizing ...... time taken: 56.15147843200003\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.922779941000044\n",
            "Completed lowercasing ...... time taken: 0.27975658299999395\n",
            "Completed tokenizing ...... time taken: 5.8538310469999715\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.85072087799995\n",
            "Completed removing punctuation ...... time taken: 6.672000358999981\n",
            "Completed tokenizing ...... time taken: 5.815043392999996\n",
            "Completed dropping empty/numerical comments ...... time taken: 1.1850183410000454\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.84222352200004\n",
            "Completed removing punctuation ...... time taken: 6.655426027999965\n",
            "Completed lowercasing ...... time taken: 0.2969079380000039\n",
            "Completed tokenizing ...... time taken: 6.145208324000009\n",
            "Completed removing stopwords ...... time taken: 4.569947154000033\n",
            "Completed lemmatizing ...... time taken: 47.41273598099997\n",
            "Completed dropping empty/numerical comments ...... time taken: 1.2421965160000354\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.86087710900006\n",
            "Completed lowercasing ...... time taken: 0.2750302599999941\n",
            "Completed tokenizing ...... time taken: 6.019081955000047\n",
            "Completed removing stopwords ...... time taken: 5.563206178000087\n",
            "Completed lemmatizing ...... time taken: 52.265339100000006\n",
            "Completed dropping empty/numerical comments ...... time taken: 1.2387559300000248\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.83023392999996\n",
            "Completed removing punctuation ...... time taken: 6.66332353100006\n",
            "Completed lowercasing ...... time taken: 0.2641134579999971\n",
            "Completed tokenizing ...... time taken: 5.488726179000082\n",
            "Completed removing stopwords ...... time taken: 5.279577621000044\n",
            "Completed stemming ...... time taken: 193.35235459399985\n",
            "Completed dropping empty/numerical comments ...... time taken: 1.1956344210000225\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Completed extracting meta data ...... time taken: 50.64237123599992\n",
            "Completed lowercasing ...... time taken: 0.27620584600003895\n",
            "Completed tokenizing ...... time taken: 6.297225735999973\n",
            "Completed lemmatizing ...... time taken: 45.29995702899987\n",
            "\n",
            "***Completed***\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results using CNN Model \n",
        "The results are done using vocab_size = 10000, embedding_dim = 16, max_length = 120, batch_size = 32, epochs = 10 on CNN Model\n",
        "\n",
        "https://colab.research.google.com/drive/1oBSJardm70rn2mzlYEfqHEmi-nFuD9lW?usp=sharing\n",
        "\n",
        "## No preprocessing\n",
        "Epoch 10/10\n",
        "25270/25270 - 97s <br>- loss: 0.4846 - accuracy: 0.7667 <br>- val_loss: 0.5580 - <b>val_accuracy: 0.7181</b> <br>- 97s/epoch - 4ms/step\n",
        "\n",
        "## lowercase_lemmatized\n",
        "Epoch 10/10\n",
        "25270/25270 - 107s <br>- loss: 0.4396 - accuracy: 0.7938 <br>- val_loss: 0.5903 - <b>val_accuracy: 0.7116</b> <br>- 107s/epoch - 4ms/step\n",
        "\n",
        "## lowercase_no_stopwords\n",
        "Epoch 10/10\n",
        "25266/25266 - 408s <br>- loss: 0.4415 - accuracy: 0.7939 <br>- val_loss: 0.6042 - <b>val_accuracy: 0.7081</b> <br>- 408s/epoch - 16ms/step\n",
        "\n",
        "## no_punctuation_numeric_empty\n",
        "Epoch 10/10\n",
        "25212/25212 - 96s <br>- loss: 0.4411 - accuracy: 0.7946 <br>- val_loss: 0.5970 - <b>val_accuracy: 0.7095</b> <br>- 96s/epoch - 4ms/step\n",
        "\n",
        "## no_puncutation\n",
        "Epoch 10/10\n",
        "25244/25244 - 106s <br>- loss: 0.4411 - accuracy: 0.7937 <br>- val_loss: 0.5997 <b>- val_accuracy: 0.7081</b> <br>- 106s/epoch - 4ms/step\n",
        "## no_pre_processing\n",
        "\n",
        "## lowercased\n",
        "\n"
      ],
      "metadata": {
        "id": "ElE9uX-4iLRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No preprocessing\n",
        "\n",
        "Epoch 1/10\n",
        "25270/25270 - 124s - loss: 0.5667 - accuracy: 0.7039 - val_loss: 0.5488 - val_accuracy: 0.7194 - 124s/epoch - 5ms/step\n",
        "Epoch 2/10\n",
        "25270/25270 - 111s - loss: 0.5362 - accuracy: 0.7292 - val_loss: 0.5431 - val_accuracy: 0.7235 - 111s/epoch - 4ms/step\n",
        "Epoch 3/10\n",
        "25270/25270 - 100s - loss: 0.5231 - accuracy: 0.7389 - val_loss: 0.5435 - val_accuracy: 0.7239 - 100s/epoch - 4ms/step\n",
        "Epoch 4/10\n",
        "25270/25270 - 98s - loss: 0.5139 - accuracy: 0.7462 - val_loss: 0.5453 - val_accuracy: 0.7240 - 98s/epoch - 4ms/step\n",
        "Epoch 5/10\n",
        "25270/25270 - 106s - loss: 0.5061 - accuracy: 0.7521 - val_loss: 0.5469 - val_accuracy: 0.7224 - 106s/epoch - 4ms/step\n",
        "Epoch 6/10\n",
        "25270/25270 - 98s - loss: 0.5004 - accuracy: 0.7563 - val_loss: 0.5513 - val_accuracy: 0.7204 - 98s/epoch - 4ms/step\n",
        "Epoch 7/10\n",
        "25270/25270 - 97s - loss: 0.4958 - accuracy: 0.7590 - val_loss: 0.5504 - val_accuracy: 0.7217 - 97s/epoch - 4ms/step\n",
        "Epoch 8/10\n",
        "25270/25270 - 111s - loss: 0.4914 - accuracy: 0.7620 - val_loss: 0.5538 - val_accuracy: 0.7204 - 111s/epoch - 4ms/step\n",
        "Epoch 9/10\n",
        "25270/25270 - 106s - loss: 0.4875 - accuracy: 0.7647 - val_loss: 0.5588 - val_accuracy: 0.7179 - 106s/epoch - 4ms/step\n",
        "Epoch 10/10\n",
        "25270/25270 - 97s - loss: 0.4846 - accuracy: 0.7667 - val_loss: 0.5580 - val_accuracy: 0.7181 - 97s/epoch - 4ms/step"
      ],
      "metadata": {
        "id": "P9i0oa1iySYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lowercase_lemmatized\n",
        "Epoch 1/10\n",
        "25270/25270 - 110s - loss: 0.5586 - accuracy: 0.7093 - val_loss: 0.5443 - val_accuracy: 0.7222 - 110s/epoch - 4ms/step\n",
        "Epoch 2/10\n",
        "25270/25270 - 106s - loss: 0.5264 - accuracy: 0.7349 - val_loss: 0.5499 - val_accuracy: 0.7163 - 106s/epoch - 4ms/step\n",
        "Epoch 3/10\n",
        "25270/25270 - 100s - loss: 0.5083 - accuracy: 0.7481 - val_loss: 0.5424 - val_accuracy: 0.7252 - 100s/epoch - 4ms/step\n",
        "Epoch 4/10\n",
        "25270/25270 - 107s - loss: 0.4935 - accuracy: 0.7586 - val_loss: 0.5473 - val_accuracy: 0.7250 - 107s/epoch - 4ms/step\n",
        "Epoch 5/10\n",
        "25270/25270 - 107s - loss: 0.4817 - accuracy: 0.7669 - val_loss: 0.5571 - val_accuracy: 0.7211 - 107s/epoch - 4ms/step\n",
        "Epoch 6/10\n",
        "25270/25270 - 98s - loss: 0.4707 - accuracy: 0.7742 - val_loss: 0.5595 - val_accuracy: 0.7180 - 98s/epoch - 4ms/step\n",
        "Epoch 7/10\n",
        "25270/25270 - 98s - loss: 0.4616 - accuracy: 0.7798 - val_loss: 0.5695 - val_accuracy: 0.7165 - 98s/epoch - 4ms/step\n",
        "Epoch 8/10\n",
        "25270/25270 - 97s - loss: 0.4533 - accuracy: 0.7853 - val_loss: 0.5704 - val_accuracy: 0.7149 - 97s/epoch - 4ms/step\n",
        "Epoch 9/10\n",
        "25270/25270 - 106s - loss: 0.4462 - accuracy: 0.7896 - val_loss: 0.5816 - val_accuracy: 0.7126 - 106s/epoch - 4ms/step\n",
        "Epoch 10/10\n",
        "25270/25270 - 107s - loss: 0.4396 - accuracy: 0.7938 - val_loss: 0.5903 - val_accuracy: 0.7116 - 107s/epoch - 4ms/step"
      ],
      "metadata": {
        "id": "pQCJIcsayZEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lowercase_no_stopwords\n",
        "Epoch 1/10\n",
        "25266/25266 - 403s - loss: 0.5612 - accuracy: 0.7073 - val_loss: 0.5451 - val_accuracy: 0.7205 - 403s/epoch - 16ms/step\n",
        "Epoch 2/10\n",
        "25266/25266 - 431s - loss: 0.5289 - accuracy: 0.7334 - val_loss: 0.5411 - val_accuracy: 0.7232 - 431s/epoch - 17ms/step\n",
        "Epoch 3/10\n",
        "25266/25266 - 419s - loss: 0.5110 - accuracy: 0.7466 - val_loss: 0.5465 - val_accuracy: 0.7228 - 419s/epoch - 17ms/step\n",
        "Epoch 4/10\n",
        "25266/25266 - 410s - loss: 0.4960 - accuracy: 0.7575 - val_loss: 0.5497 - val_accuracy: 0.7211 - 410s/epoch - 16ms/step\n",
        "Epoch 5/10\n",
        "25266/25266 - 414s - loss: 0.4837 - accuracy: 0.7665 - val_loss: 0.5622 - val_accuracy: 0.7183 - 414s/epoch - 16ms/step\n",
        "Epoch 6/10\n",
        "25266/25266 - 423s - loss: 0.4728 - accuracy: 0.7739 - val_loss: 0.5593 - val_accuracy: 0.7170 - 423s/epoch - 17ms/step\n",
        "Epoch 7/10\n",
        "25266/25266 - 416s - loss: 0.4635 - accuracy: 0.7803 - val_loss: 0.5694 - val_accuracy: 0.7126 - 416s/epoch - 16ms/step\n",
        "Epoch 8/10\n",
        "25266/25266 - 408s - loss: 0.4553 - accuracy: 0.7849 - val_loss: 0.5726 - val_accuracy: 0.7110 - 408s/epoch - 16ms/step\n",
        "Epoch 9/10\n",
        "25266/25266 - 418s - loss: 0.4479 - accuracy: 0.7898 - val_loss: 0.5975 - val_accuracy: 0.7103 - 418s/epoch - 17ms/step\n",
        "Epoch 10/10\n",
        "25266/25266 - 408s - loss: 0.4415 - accuracy: 0.7939 - val_loss: 0.6042 - val_accuracy: 0.7081 - 408s/epoch - 16ms/step"
      ],
      "metadata": {
        "id": "SG4aKeb93vsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no_punctuation_numeric_empty\n",
        "Epoch 1/10\n",
        "25212/25212 - 107s - loss: 0.5603 - accuracy: 0.7084 - val_loss: 0.5430 - val_accuracy: 0.7228 - 107s/epoch - 4ms/step\n",
        "Epoch 2/10\n",
        "25212/25212 - 106s - loss: 0.5286 - accuracy: 0.7333 - val_loss: 0.5392 - val_accuracy: 0.7251 - 106s/epoch - 4ms/step\n",
        "Epoch 3/10\n",
        "25212/25212 - 97s - loss: 0.5108 - accuracy: 0.7468 - val_loss: 0.5413 - val_accuracy: 0.7260 - 97s/epoch - 4ms/step\n",
        "Epoch 4/10\n",
        "25212/25212 - 97s - loss: 0.4962 - accuracy: 0.7575 - val_loss: 0.5505 - val_accuracy: 0.7203 - 97s/epoch - 4ms/step\n",
        "Epoch 5/10\n",
        "25212/25212 - 97s - loss: 0.4838 - accuracy: 0.7661 - val_loss: 0.5553 - val_accuracy: 0.7176 - 97s/epoch - 4ms/step\n",
        "Epoch 6/10\n",
        "25212/25212 - 98s - loss: 0.4731 - accuracy: 0.7735 - val_loss: 0.5598 - val_accuracy: 0.7163 - 98s/epoch - 4ms/step\n",
        "Epoch 7/10\n",
        "25212/25212 - 95s - loss: 0.4639 - accuracy: 0.7794 - val_loss: 0.5708 - val_accuracy: 0.7155 - 95s/epoch - 4ms/step\n",
        "Epoch 8/10\n",
        "25212/25212 - 97s - loss: 0.4555 - accuracy: 0.7856 - val_loss: 0.5782 - val_accuracy: 0.7141 - 97s/epoch - 4ms/step\n",
        "Epoch 9/10\n",
        "25212/25212 - 107s - loss: 0.4480 - accuracy: 0.7898 - val_loss: 0.5889 - val_accuracy: 0.7097 - 107s/epoch - 4ms/step\n",
        "Epoch 10/10\n",
        "25212/25212 - 96s - loss: 0.4411 - accuracy: 0.7946 - val_loss: 0.5970 - val_accuracy: 0.7095 - 96s/epoch - 4ms/step"
      ],
      "metadata": {
        "id": "g0yfgz2CyhTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no_puncutation\n",
        "Epoch 1/10\n",
        "25244/25244 - 98s - loss: 0.5596 - accuracy: 0.7092 - val_loss: 0.5450 - val_accuracy: 0.7201 - 98s/epoch - 4ms/step\n",
        "Epoch 2/10\n",
        "25244/25244 - 106s - loss: 0.5265 - accuracy: 0.7348 - val_loss: 0.5431 - val_accuracy: 0.7226 - 106s/epoch - 4ms/step\n",
        "Epoch 3/10\n",
        "25244/25244 - 107s - loss: 0.5089 - accuracy: 0.7480 - val_loss: 0.5437 - val_accuracy: 0.7233 - 107s/epoch - 4ms/step\n",
        "Epoch 4/10\n",
        "25244/25244 - 97s - loss: 0.4946 - accuracy: 0.7576 - val_loss: 0.5503 - val_accuracy: 0.7200 - 97s/epoch - 4ms/step\n",
        "Epoch 5/10\n",
        "25244/25244 - 98s - loss: 0.4825 - accuracy: 0.7665 - val_loss: 0.5644 - val_accuracy: 0.7161 - 98s/epoch - 4ms/step\n",
        "Epoch 6/10\n",
        "25244/25244 - 105s - loss: 0.4724 - accuracy: 0.7729 - val_loss: 0.5623 - val_accuracy: 0.7149 - 105s/epoch - 4ms/step\n",
        "Epoch 7/10\n",
        "25244/25244 - 107s - loss: 0.4629 - accuracy: 0.7793 - val_loss: 0.5702 - val_accuracy: 0.7134 - 107s/epoch - 4ms/step\n",
        "Epoch 8/10\n",
        "25244/25244 - 106s - loss: 0.4549 - accuracy: 0.7849 - val_loss: 0.5845 - val_accuracy: 0.7108 - 106s/epoch - 4ms/step\n",
        "Epoch 9/10\n",
        "25244/25244 - 106s - loss: 0.4476 - accuracy: 0.7896 - val_loss: 0.5936 - val_accuracy: 0.7098 - 106s/epoch - 4ms/step\n",
        "Epoch 10/10\n",
        "25244/25244 - 106s - loss: 0.4411 - accuracy: 0.7937 - val_loss: 0.5997 - val_accuracy: 0.7081 - 106s/epoch - 4ms/step"
      ],
      "metadata": {
        "id": "dKGMqKegytCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lowercased\n",
        "\n",
        "Epoch 1/10\n",
        "25270/25270 - 100s - loss: 0.5588 - accuracy: 0.7092 - val_loss: 0.5441 - val_accuracy: 0.7219 - 100s/epoch - 4ms/step\n",
        "Epoch 2/10\n",
        "25270/25270 - 105s - loss: 0.5259 - accuracy: 0.7352 - val_loss: 0.5393 - val_accuracy: 0.7265 - 105s/epoch - 4ms/step\n",
        "Epoch 3/10\n",
        "25270/25270 - 106s - loss: 0.5074 - accuracy: 0.7490 - val_loss: 0.5420 - val_accuracy: 0.7267 - 106s/epoch - 4ms/step\n",
        "Epoch 4/10\n",
        "25270/25270 - 104s - loss: 0.4927 - accuracy: 0.7595 - val_loss: 0.5476 - val_accuracy: 0.7234 - 104s/epoch - 4ms/step\n",
        "Epoch 5/10\n",
        "25270/25270 - 97s - loss: 0.4804 - accuracy: 0.7681 - val_loss: 0.5549 - val_accuracy: 0.7223 - 97s/epoch - 4ms/step\n",
        "Epoch 6/10\n",
        "25270/25270 - 105s - loss: 0.4695 - accuracy: 0.7750 - val_loss: 0.5577 - val_accuracy: 0.7197 - 105s/epoch - 4ms/step\n",
        "Epoch 7/10\n",
        "25270/25270 - 96s - loss: 0.4602 - accuracy: 0.7808 - val_loss: 0.5708 - val_accuracy: 0.7170 - 96s/epoch - 4ms/step\n",
        "Epoch 8/10\n",
        "25270/25270 - 107s - loss: 0.4522 - accuracy: 0.7863 - val_loss: 0.5718 - val_accuracy: 0.7148 - 107s/epoch - 4ms/step\n",
        "Epoch 9/10\n",
        "25270/25270 - 106s - loss: 0.4448 - accuracy: 0.7907 - val_loss: 0.5835 - val_accuracy: 0.7140 - 106s/epoch - 4ms/step\n",
        "Epoch 10/10\n",
        "25270/25270 - 98s - loss: 0.4384 - accuracy: 0.7946 - val_loss: 0.5988 - val_accuracy: 0.7121 - 98s/epoch - 4ms/step"
      ],
      "metadata": {
        "id": "WFKITQ6V3k12"
      }
    }
  ]
}